# ğŸš€ Instructions d'utilisation - TP RAG

## ğŸ“‹ PrÃ©requis

### Installation locale

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/vincentmartin/tp-rag-student-version.git
cd tp-rag-student-version

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Installation sur Google Colab

Le notebook gÃ¨re automatiquement l'installation. ExÃ©cutez simplement la premiÃ¨re cellule.

## ğŸ”§ Configuration d'Ollama

### Sur machine locale

```bash
# Installation d'Ollama
curl https://ollama.ai/install.sh | sh

# Lancement du serveur
ollama serve

# Dans un autre terminal, tÃ©lÃ©charger le modÃ¨le
ollama pull qwen2.5:3b
```

### Sur Google Colab

ExÃ©cutez ces commandes dans le terminal Colab (via colab-xterm) :

```bash
curl https://ollama.ai/install.sh | sh
ollama serve &
ollama pull qwen2.5:3b
```

## ğŸ“Š Structure du projet

```
tp-rag-student-version/
â”œâ”€â”€ TP.ipynb                    # Notebook principal avec tous les exercices
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ README.md                   # Consignes du TP
â”œâ”€â”€ INSTRUCTIONS.md            # Ce fichier
â”œâ”€â”€ data/                      # Documents Ã  indexer
â”‚   â””â”€â”€ arxiv/                 # PDFs scientifiques
â”œâ”€â”€ chroma_db/                 # Base vectorielle (crÃ©Ã©e automatiquement)
â””â”€â”€ multi_agent_data/          # Ressources pour exercice 8
    â”œâ”€â”€ COURS_MULTI_AGENTS_DATA.md
    â””â”€â”€ notebooks/
```

## ğŸ¯ Exercices implÃ©mentÃ©s

### âœ… Exercice 1 : Indexation
- Chargement des PDFs avec PyPDFDirectoryLoader
- DÃ©coupage en chunks (1000 tokens, overlap 200)
- Embeddings avec multilingual-e5-base
- Stockage dans ChromaDB

### âœ… Exercice 2 : Interrogation
- Fonction `search_documents()` avec scores de similaritÃ©
- Affichage formatÃ© des rÃ©sultats

### âœ… Exercice 3 : Prompt Template
- Template optimisÃ© pour RAG
- Instructions claires pour l'utilisation du contexte

### âœ… Exercice 4 : ChaÃ®ne RAG
- IntÃ©gration Ollama/Qwen
- ChaÃ®ne complÃ¨te avec retriever + LLM
- Fonction `ask_question()` pour tester

### âœ… Exercice 5 : MÃ©moire
- Gestion de l'historique avec ChatMessageHistory
- RunnableWithMessageHistory pour le suivi
- Fonction `chat_with_memory()` pour conversations

### âœ… Exercice 6 : RÃ©sumÃ© de documents
- Fonction `summarize_document()` avec map_reduce
- Support de documents complets

### âœ… Exercice 7 : IHM Gradio
- Interface complÃ¨te avec historique
- Gestion des sessions
- Design moderne et intuitif

### ğŸ“š Exercice 8 : Ã‰valuation
- Ã€ rÃ©aliser dans le notebook L6
- IntÃ©gration RAG local + DuckDB
- Ã‰valuation avec TruLens

### ğŸŒ Exercice 9 : Recherche Web (Optionnel)
- Alternative DuckDuckGo Ã  Tavily

## ğŸ® Utilisation

### Mode Notebook

```python
# 1. ExÃ©cuter les cellules dans l'ordre
# 2. Tester la recherche
results = search_documents("What is RAG?", k=3)

# 3. Poser une question
response = ask_question("Explain multi-agent systems")

# 4. Conversation avec mÃ©moire
chat_with_memory("What are the challenges?", session_id="my_session")
chat_with_memory("Can you elaborate?", session_id="my_session")

# 5. RÃ©sumer un document
summarize_document("data/arxiv/Complex_QA_and_language_models_hybrid_architectures_Survey.pdf")
```

### Mode Interface Gradio

ExÃ©cutez la cellule finale pour lancer l'interface web.
Un lien sera gÃ©nÃ©rÃ© (avec `share=True` sur Colab, vous aurez un lien public).

## âš™ï¸ Configuration avancÃ©e

### Modifier le chunk size

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Augmenter pour plus de contexte
    chunk_overlap=300,
    length_function=len,
)
```

### Changer le nombre de documents rÃ©cupÃ©rÃ©s

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 6}  # Plus de documents
)
```

### Utiliser un modÃ¨le diffÃ©rent

```python
llm = ChatOllama(
    model="llama3.1:8b",  # Autre modÃ¨le
    temperature=0.2,  # Plus de crÃ©ativitÃ©
)
```

## ğŸ› RÃ©solution de problÃ¨mes

### Erreur "Ollama not found"

```bash
# VÃ©rifier qu'Ollama est installÃ©
which ollama

# VÃ©rifier que le serveur tourne
curl http://localhost:11434/api/tags
```

### Erreur de mÃ©moire (OOM)

- Utilisez un modÃ¨le plus petit : `qwen2.5:1.5b`
- RÃ©duisez le nombre de chunks : `k=2`
- RÃ©duisez le chunk_size : `800`

### ChromaDB dÃ©jÃ  existant

```python
# Supprimer et recrÃ©er
import shutil
shutil.rmtree("chroma_db")
```

### GPU non dÃ©tectÃ©

```python
# Pour les embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-base",
    model_kwargs={'device': 'cuda'}  # Forcer CUDA
)
```

## ğŸ“Š MÃ©triques de performance

### Temps d'indexation typique
- 10 PDFs (~100 pages) : ~2-3 minutes
- DÃ©pend de la machine et du CPU

### Temps de rÃ©ponse
- Recherche vectorielle : <1 seconde
- GÃ©nÃ©ration LLM : 3-10 secondes (selon modÃ¨le)
- Total : ~5-15 secondes par question

## ğŸ“ Ressources supplÃ©mentaires

- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [ChromaDB Guide](https://docs.trychroma.com/getting-started)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Gradio Guide](https://www.gradio.app/guides/quickstart)
- [TruLens RAG Evaluation](https://www.trulens.org/trulens_eval/getting_started/quickstarts/quickstart/)

## ğŸ’¡ Conseils pour le rendu

1. **Code propre** : Respecter PEP 8, commentaires clairs
2. **Documentation** : Docstrings pour toutes les fonctions
3. **Tests** : Montrer que chaque exercice fonctionne
4. **Ã‰valuation** : L'exercice 8 est important pour la note
5. **CrÃ©ativitÃ©** : Ajouter des fonctionnalitÃ©s bonus

## ğŸ“§ Support

Pour toute question, rÃ©fÃ©rez-vous au README.md principal ou contactez votre enseignant.

---

**Bon TP ! ğŸš€**
